{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from coca_transformer import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_visual_input = torch.randn(2, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoCa Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_visual = VisionTransformer(\n",
    "    image_size = 32,\n",
    "    in_channels = 3,\n",
    "    patch_size = 4,\n",
    "    width = 128,\n",
    "    layers = 4,\n",
    "    heads = 8,\n",
    "    mlp_ratio = 4,\n",
    "    attentional_pool = True\n",
    ")\n",
    "output = coca_visual(_visual_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "torch.Size([2, 255, 512])\n"
     ]
    }
   ],
   "source": [
    "for _out in output:\n",
    "    print(_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from coca_transformer import Attention\n",
    "\n",
    "attn = Attention(dim = 128)\n",
    "attn(torch.randn(2, 4, 128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoCa Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coca_transformer import TextTransformer\n",
    "\n",
    "text_trn = TextTransformer()\n",
    "txt_output = text_trn(torch.randint(0, 128, (2, 77)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 512]), torch.Size([2, 77, 512]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_output[0].shape, txt_output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coca_tokenzier import SimpleTokenizer, decode, tokenize\n",
    "\n",
    "tokenzier = SimpleTokenizer()\n",
    "result = tokenize(\n",
    "    tokenzier,\n",
    "    [\"This image is nuisance, with low signal.\",\n",
    "     \"This image is defect, with low signal.\",\n",
    "     \"abda haos aoej\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49406,   596,  1140,   560,  1299,  7313, 25009, 49407,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoCa Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coca_cfg import CLIPTextCfg, CLIPVisionCfg, MultimodalCfg\n",
    "from coca_model import _build_vision_tower, _build_text_tower, _build_text_decoder_tower\n",
    "from coca_model import CoCa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n",
      "torch.Size([3, 255, 512])\n"
     ]
    }
   ],
   "source": [
    "clip_visual_cfg = CLIPVisionCfg(in_channels = 4)\n",
    "coca_visual = _build_vision_tower(512, clip_visual_cfg)\n",
    "visual_output = coca_visual(torch.randn(3, 4, 32, 32))\n",
    "for _out in visual_output:\n",
    "    print(_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n",
      "torch.Size([3, 77, 512])\n"
     ]
    }
   ],
   "source": [
    "clip_text_cfg = CLIPTextCfg()\n",
    "coca_text = _build_text_tower(512, clip_text_cfg)\n",
    "text_output = coca_text(torch.randint(0, 128, (3, 77)))\n",
    "for _out in text_output:\n",
    "    print(_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 512])\n",
      "torch.Size([77, 512])\n",
      "torch.Size([77, 512])\n"
     ]
    }
   ],
   "source": [
    "clip_multi_cfg = MultimodalCfg()\n",
    "coca_decoder = _build_text_decoder_tower(512, clip_multi_cfg)\n",
    "coca_output = coca_decoder(visual_output[1], text_output[1])\n",
    "for _out in coca_output:\n",
    "    print(_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_main_model = CoCa(\n",
    "    embed_dim = 512,\n",
    "    multimodal_cfg = MultimodalCfg(),\n",
    "    text_cfg = CLIPTextCfg(),\n",
    "    vision_cfg = CLIPVisionCfg(in_channels = 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_result = coca_main_model(\n",
    "    image = torch.randn(2, 4, 32, 32),\n",
    "    text = torch.randint(0, 40000, (2, 77))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features: torch.Size([2, 512])\n",
      "text_features: torch.Size([2, 512])\n",
      "logits: torch.Size([2, 76, 49408])\n",
      "labels: torch.Size([2, 76])\n",
      "logit_scale: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "for k, v in coca_result.items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################!\n"
     ]
    }
   ],
   "source": [
    "coca_txt_result = coca_main_model.generate(torch.randn(1, 4, 32, 32) * 0.2)\n",
    "for txt_result in coca_txt_result:\n",
    "    print(decode(txt_result).split(\"<|endoftext|>\")[0].replace(\"<|startoftext|>\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoCa Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coca_loss import CoCaLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6957, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coca_criterion = CoCaLoss(caption_loss_weight = 0.5, clip_loss_weight = 0.5)\n",
    "losses = coca_criterion(\n",
    "    image_features = coca_result['image_features'], \n",
    "    text_features = coca_result['text_features'], \n",
    "    logits = coca_result['logits'], \n",
    "    labels = coca_result['labels'], \n",
    "    logit_scale = coca_result['logit_scale'],\n",
    "    output_dict = True\n",
    ")\n",
    "total_loss = sum(losses.values())\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoCa Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CoCaDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.image = torch.randn(50, 4, 32, 32)\n",
    "        self.text = torch.randint(0, 40000, (200, 77))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image[idx], self.text[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Data (t): 0.001\n",
      "contrastive_loss: 1.181,caption_loss: 5.404,loss: 6.585,\n",
      "Train Epoch: 1, Data (t): 0.020\n",
      "contrastive_loss: 1.234,caption_loss: 5.404,loss: 6.638,\n",
      "Train Epoch: 1, Data (t): 0.029\n",
      "contrastive_loss: 1.250,caption_loss: 5.404,loss: 6.654,\n",
      "Train Epoch: 1, Data (t): 0.032\n",
      "contrastive_loss: 1.238,caption_loss: 5.404,loss: 6.642,\n",
      "Train Epoch: 1, Data (t): 0.033\n",
      "contrastive_loss: 1.241,caption_loss: 5.404,loss: 6.645,\n"
     ]
    }
   ],
   "source": [
    "from coca_train import train_one_epoch\n",
    "\n",
    "\n",
    "coca_main_model = CoCa(\n",
    "    embed_dim = 512,\n",
    "    multimodal_cfg = MultimodalCfg(),\n",
    "    text_cfg = CLIPTextCfg(),\n",
    "    vision_cfg = CLIPVisionCfg(in_channels = 4)\n",
    ")\n",
    "caca_dataloader = DataLoader(\n",
    "    CoCaDataset(),\n",
    "    batch_size = 10\n",
    ")\n",
    "optimizer = torch.optim.SGD(coca_main_model.parameters(), lr = 1e-3, momentum = 0.95)\n",
    "\n",
    "train_one_epoch(\n",
    "    model = coca_main_model, \n",
    "    dataloader = caca_dataloader, \n",
    "    loss = coca_criterion, \n",
    "    epoch = 1, \n",
    "    optimizer = optimizer, \n",
    "    scaler = None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7acebdac238784abba9fd84790e334c9ad79b6d735ecdb94eed7c00fde5e1647"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('yolov5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
